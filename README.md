å¥½ï¼Œè¿™é‡Œæˆ‘ç»™ä½ ä¸€ä»½**å¹²å‡€ã€ä¸“ä¸šã€åç ”ç©¶/æ•™å­¦ç”¨é€”çš„ README**ï¼Œå®Œå…¨è´´åˆä½ çŽ°åœ¨è¿™æ®µä»£ç å’Œä½ æ­£åœ¨å­¦çš„å†…å®¹ï¼ˆMLE + log-likelihoodï¼‰ã€‚
ä½ å¯ä»¥ç›´æŽ¥æ”¾åœ¨ GitHub ä»“åº“é‡Œç”¨ã€‚

---

# Maximum Likelihood Estimation for a Biased Coin (Python)

## ðŸ“Œ Project Overview

This project demonstrates **Maximum Likelihood Estimation (MLE)** for estimating the probability of heads in a biased coin experiment using **log-likelihood**.

We simulate a large number of coin tosses, construct the log-likelihood function of the Bernoulli/Binomial model, and numerically identify the value of the parameter that maximizes it.

The goal of this project is **conceptual clarity**, not performance optimization.

---

## ðŸ§  Statistical Background

We assume:

* Each coin toss follows a **Bernoulli distribution**
* The probability of heads is an unknown parameter ( p \in (0,1) )
* Given observed data, we estimate ( p ) using **maximum likelihood**

For ( N ) tosses with:

* ( H ) heads
* ( T = N - H ) tails

The **log-likelihood function** is:

[
\log L(p)
=========

H \log(p) + T \log(1 - p)
]

The MLE for ( p ) is known analytically to be:

[
\hat{p} = \frac{H}{N}
]

This project verifies that result **numerically**.

---

## ðŸ§ª What the Code Does

1. Simulates `N = 10,000` coin tosses from a biased coin
2. Computes the number of heads and tails
3. Defines the **log-likelihood function**
4. Evaluates the log-likelihood over a grid of candidate ( p ) values
5. Identifies the **maximum likelihood estimate**
6. Visualizes the log-likelihood curve

---

## ðŸ“‚ File Structure

```
.
â”œâ”€â”€ mle_coin.py        # Main script
â”œâ”€â”€ README.md          # Project documentation
```

---

## â–¶ï¸ How to Run

### Requirements

* Python 3.8+
* NumPy
* Matplotlib

Install dependencies if needed:

```bash
pip install numpy matplotlib
```

### Run the script

```bash
python mle_coin.py
```

---

## ðŸ“ˆ Example Output

* Printed output:

  * Number of heads and tails
  * Estimated MLE for ( p )

* Plot:

  * Log-likelihood as a function of ( p )
  * A clear single maximum near the true probability

---

## ðŸ” Key Implementation Details

* **Log-likelihood is used instead of likelihood** to avoid numerical underflow
* Invalid parameter values (`p â‰¤ 0` or `p â‰¥ 1`) return `-âˆž` to enforce parameter constraints
* The MLE is found using `argmax`, not closed-form formulas, to emphasize generality

---

## ðŸ§© Why This Matters

This example illustrates core ideas used throughout:

* Statistical inference
* Numerical optimization
* EM algorithms
* Regime-switching and mixture models
* Quantitative finance and econometrics

Understanding this simple case makes more complex models (e.g. Gaussian mixtures, hidden states, EM) much easier to grasp.

---

## ðŸš€ Possible Extensions

* Derive the MLE analytically using first-order conditions
* Replace grid search with `scipy.optimize`
* Extend to a **two-coin mixture model** and implement EM
* Add confidence intervals using Fisher Information

---

## ðŸ§  Takeaway

> **Maximum likelihood estimation is simple in theory,
> but only works in practice when implemented with numerical care.**

This project focuses on building that care from the ground up.


